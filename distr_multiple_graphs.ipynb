{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed processing to obtain graphs from all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readfof\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/04 10:18:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"CosmoSparkApplication\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def read_cosmo_data(file_path):\n",
    "\n",
    "    # Read Fof\n",
    "    FoF = readfof.FoF_catalog(\n",
    "        file_path,           # simulation directory\n",
    "        2,                   # snapnum, indicating the redshift (z=1)\n",
    "        long_ids = False,\n",
    "        swap = False,\n",
    "        SFR = False,\n",
    "        read_IDs = False\n",
    "        )\n",
    "\n",
    "    return FoF\n",
    "\n",
    "\n",
    "# Get masses and positions from FoF\n",
    "def get_pos_mass(FoF):\n",
    "\n",
    "    pos = FoF.GroupPos/1e06             # Halo positions in Gpc/h \n",
    "    mass_raw = FoF.GroupMass * 1e10     # Halo masses in Msun/h\n",
    "\n",
    "    dim = pos.shape[0]\n",
    "    pos_mass_matrix = np.hstack([pos, mass_raw.reshape(dim, 1)])\n",
    "\n",
    "    return pos_mass_matrix\n",
    "\n",
    "# To assign simulation keys to each point in each simulation\n",
    "def assign_key_to_rows(key_value_pair):\n",
    "    key, array = key_value_pair\n",
    "    return [(key, row) for row in array]\n",
    "\n",
    "\n",
    "# Plot a graph in 3D space\n",
    "def plot_graph_3D(num, pars_file, pos, masses, edge_idx):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fontsize = 12\n",
    "\n",
    "    ax = fig.add_subplot(projection =\"3d\")\n",
    "\n",
    "    pos = np.array(pos, dtype=float) * 1.e3   # show in Mpc\n",
    "\n",
    "    # Draw lines for each edge\n",
    "    for (src, dst) in edge_idx: #.t().tolist():\n",
    "\n",
    "        src = pos[int(src)].tolist()\n",
    "        dst = pos[int(dst)].tolist()\n",
    "\n",
    "        ax.plot([src[0], dst[0]], [src[1], dst[1]], zs=[src[2], dst[2]], linewidth=0.6, color='dimgrey')\n",
    "\n",
    "    # Plot nodes\n",
    "    mass_mean = np.mean(masses)\n",
    "    for i,m in enumerate(masses):\n",
    "            ax.scatter(pos[i, 0], pos[i, 1], pos[i, 2], s=50*m*m/(mass_mean**2), zorder=1000, alpha=0.6, color = 'mediumpurple')\n",
    "\n",
    "    ax.xaxis.set_tick_params(labelsize=fontsize)\n",
    "    ax.yaxis.set_tick_params(labelsize=fontsize)\n",
    "    ax.zaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "    ax.set_xlabel('x (Mpc)', fontsize=16, labelpad=15)\n",
    "    ax.set_ylabel('y (Mpc)', fontsize=16, labelpad=15)\n",
    "    ax.set_zlabel('z (Mpc)', fontsize=16, labelpad=15)\n",
    "\n",
    "    rl = '$R_{link} = 0.2$'\n",
    "\n",
    "    pars_file = pars_file[num]\n",
    "\n",
    "    ax.set_title(f'\\tGraph nÂ°{num}, Masses $\\\\geq 99.7$% percentile, {rl} Mpc \\t \\n \\n $\\\\Omega_m = {float(pars_file[0]):.3f}$ \\t $\\\\sigma_8 = {float(pars_file[1]):.3f}$', fontsize=20)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Graph object\n",
    "class graph:\n",
    "\n",
    "    def __init__(self, node_f, pos, sim_pars, glob_f, edge_idx, edge_f):\n",
    "        \n",
    "        self.node_f = node_f\n",
    "        self.pos = pos\n",
    "        self.sim_pars = sim_pars\n",
    "        self.glob_f = glob_f\n",
    "        self.edge_idx = edge_idx\n",
    "        self.edge_f = edge_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read simulations and parallelize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# simulations parameter\n",
    "sim_pars_file = np.loadtxt(\"/mnt/cosmo_GNN/latin_hypercube_params.txt\", dtype=float)\n",
    "\n",
    "# number of simulations to be processed\n",
    "n_sims = 10\n",
    "\n",
    "# path list with simulation keys\n",
    "path_list = [(i, \"/mnt/cosmo_GNN/Data/\" + str(i)) for i in range(n_sims)]\n",
    "\n",
    "# parallelize path list and read files\n",
    "fof_rdd = sc.parallelize(path_list)\\\n",
    "            .mapValues(read_cosmo_data)\n",
    "\n",
    "# get positions and masses for each point\n",
    "pos_mass_rdd = fof_rdd.mapValues(get_pos_mass)\\\n",
    "                      .flatMap(assign_key_to_rows)\n",
    "\n",
    "# cut percentile\n",
    "cut = 0.997\n",
    "\n",
    "# get mass cuts \n",
    "mass_cut_rdd = fof_rdd.mapValues(get_pos_mass)\\\n",
    "                      .mapValues(lambda x: np.quantile(x[:, -1], cut))\n",
    "\n",
    "mass_cuts = mass_cut_rdd.values().collect()\n",
    "mass_cuts = np.array(mass_cuts)\n",
    "\n",
    "# filter by mass\n",
    "pos_mass_rdd_filtered = pos_mass_rdd.filter(lambda x: x[1][-1] >= mass_cuts[x[0]])\n",
    "\n",
    "# number of halos in each simulation\n",
    "n_halos = pos_mass_rdd_filtered.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  array([6.7691261e-01, 6.6048630e-02, 2.6853576e-01, 6.3511346e+14],\n",
       "        dtype=float32)),\n",
       " (0,\n",
       "  array([1.6845600e-01, 6.2195367e-01, 3.8025469e-01, 4.6018135e+14],\n",
       "        dtype=float32)),\n",
       " (0,\n",
       "  array([3.1951472e-01, 1.5889315e-01, 3.7413445e-01, 4.3477326e+14],\n",
       "        dtype=float32))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_mass_rdd_filtered.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {0: 188,\n",
       "             1: 641,\n",
       "             2: 248,\n",
       "             3: 517,\n",
       "             4: 449,\n",
       "             5: 681,\n",
       "             6: 220,\n",
       "             7: 222,\n",
       "             8: 478,\n",
       "             9: 179})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_halos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering phase to get linked halos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masses rdd\n",
    "mass_rdd = pos_mass_rdd_filtered.mapValues(lambda x: x[:3])\n",
    "\n",
    "# positions rdd\n",
    "pos_rdd = pos_mass_rdd_filtered.mapValues(lambda x: x[:3])\n",
    "\n",
    "# indexed positions rdd\n",
    "idx_pos_rdd = pos_rdd.zipWithIndex()\\\n",
    "                     .map(lambda x: (x[1], x[0]))\n",
    "\n",
    "#cartesian product between the positions rdd and itself (to obtain all the possible pairs)\n",
    "cartesian_rdd = idx_pos_rdd.cartesian(idx_pos_rdd)\n",
    "\n",
    "# compute differences between every pair\n",
    "diff_rdd = cartesian_rdd.map(lambda x: (x[0][0], x[0][1][0], x[1][1][0], np.abs(x[0][1][1][1] - x[1][1][1])))\n",
    "\n",
    "# compute distances between every pair\n",
    "# pairs_dist_rdd = diff_rdd.map(lambda x: (x[0], x[1], np.linalg.norm(x[2])))\n",
    "\n",
    "# # pairs and distances rdd filtered (by linking radius)\n",
    "# linked_pairs_dist_rdd = pairs_dist_rdd.filter(lambda x: x[2] <= 0.2)\n",
    "\n",
    "# # pairs rdd\n",
    "# pairs_rdd = linked_pairs_dist_rdd.map(lambda x: (x[0], x[1]))\n",
    "\n",
    "# # distances rdd \n",
    "# distances_rdd = linked_pairs_dist_rdd.map(lambda x: x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[((0, (0, array([0.6769126 , 0.06604863, 0.26853576], dtype=float32))),\n",
       "  (0, (0, array([0.6769126 , 0.06604863, 0.26853576], dtype=float32)))),\n",
       " ((0, (0, array([0.6769126 , 0.06604863, 0.26853576], dtype=float32))),\n",
       "  (1, (0, array([0.168456  , 0.62195367, 0.3802547 ], dtype=float32)))),\n",
       " ((0, (0, array([0.6769126 , 0.06604863, 0.26853576], dtype=float32))),\n",
       "  (2, (0, array([0.31951472, 0.15889315, 0.37413445], dtype=float32))))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartesian_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0, array([0.610864  , 0.        , 0.20248714], dtype=float32)),\n",
       " (0, 0, 0, array([0.10240737, 0.55590504, 0.31420606], dtype=float32)),\n",
       " (0, 0, 0, array([0.2534661 , 0.09284452, 0.30808583], dtype=float32)),\n",
       " (0, 0, 0, array([0.47995615, 0.7942723 , 0.23586324], dtype=float32)),\n",
       " (0, 0, 0, array([0.1561198 , 0.61533946, 0.6963896 ], dtype=float32))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_rdd.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A graph 3D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving edge features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global features and graph objects creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Spark Context and Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
