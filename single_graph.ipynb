{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readfof\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import scipy.spatial as SS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/02 11:03:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"CosmoSparkApplication\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def read_cosmo_data(file_path):\n",
    "\n",
    "    # Read Fof\n",
    "    FoF = readfof.FoF_catalog(\n",
    "        file_path,           # simulation directory\n",
    "        2,                   # snapnum, indicating the redshift (z=1)\n",
    "        long_ids = False,\n",
    "        swap = False,\n",
    "        SFR = False,\n",
    "        read_IDs = False\n",
    "        )\n",
    "\n",
    "    return FoF\n",
    "\n",
    "# Get masses and positions from FoF\n",
    "def get_pos_mass(FoF):\n",
    "\n",
    "    pos = FoF.GroupPos/1e06             # Halo positions in Gpc/h \n",
    "    mass_raw = FoF.GroupMass * 1e10     # Halo masses in Msun/h\n",
    "\n",
    "    dim = pos.shape[0]\n",
    "    pos_mass_matrix = np.hstack([pos, mass_raw.reshape(dim, 1)])\n",
    "\n",
    "    return pos_mass_matrix\n",
    "\n",
    "# Mass cut function\n",
    "def mass_filter(pos_mass_element, cut):\n",
    "\n",
    "    mass = pos_mass_element[3]\n",
    "    if mass >= cut:\n",
    "        return pos_mass_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations parameter\n",
    "\n",
    "sim_pars_file = np.loadtxt(\"/mnt/cosmo_GNN/latin_hypercube_params.txt\", dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read file and get positions and masses\n",
    "file_path = \"/mnt/cosmo_GNN/Data/\" + str(13)\n",
    "test_FoF = read_cosmo_data(file_path)\n",
    "pos_mass_array = get_pos_mass(test_FoF)\n",
    "\n",
    "# mass cut\n",
    "cut = np.quantile(pos_mass_array[:, 3], 0.997)\n",
    "\n",
    "# parallelize and filter\n",
    "pos_mass_rdd = sc.parallelize(pos_mass_array)\\\n",
    "                .map(lambda el: mass_filter(el, cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs(array):\n",
    "\n",
    "    distances = []\n",
    "    for i in range(array.shape[0]):\n",
    "        dist = []\n",
    "        for j in range(i, array.shape[0]):\n",
    "            diff = np.abs(array[i]-array[j])\n",
    "            dist.append(np.linalg.norm(diff))\n",
    "        distances.append(dist)\n",
    "\n",
    "    pairs = []\n",
    "    for i in range(len(distances)):\n",
    "        for j in range(1, len(distances[i])):\n",
    "            if distances[i][j] <= 0.2:\n",
    "                pairs.append([i,j])\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pairs_distr(rdd):\n",
    "\n",
    "    # Step 1: Create all pairs of indices using the Cartesian product\n",
    "    indexed_rdd = rdd.zipWithIndex()\n",
    "    cartesian_rdd = indexed_rdd.cartesian(indexed_rdd)\n",
    "    \n",
    "    # Step 2: Compute distances for each pair\n",
    "    distances_rdd = cartesian_rdd.map(lambda x: (x[0][1], x[1][1], np.linalg.norm(np.abs(x[0][0] - x[1][0]))))\n",
    "    \n",
    "    # Step 3: Filter pairs based on distance <= 0.2\n",
    "    close_pairs_rdd = distances_rdd.filter(lambda x: x[0] < x[1] and x[2] <= 0.2)\n",
    "    \n",
    "    # Step 4: Collect and return the pairs\n",
    "    pairs = close_pairs_rdd.map(lambda x: (x[0], x[1])).collect()\n",
    "    \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([7.4910957e-01, 3.9241824e-01, 7.2230619e-01, 1.3495033e+15],\n",
       "        dtype=float32),\n",
       "  0),\n",
       " (array([5.2160764e-01, 8.4915513e-01, 7.3328722e-01, 1.1827261e+15],\n",
       "        dtype=float32),\n",
       "  1),\n",
       " (array([3.10260355e-01, 2.15019077e-01, 3.10398489e-02, 1.08891796e+15],\n",
       "        dtype=float32),\n",
       "  2)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_mass_rdd.zipWithIndex().take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([7.4910957e-01, 3.9241824e-01, 7.2230619e-01, 1.3495033e+15],\n",
       "       dtype=float32),\n",
       " array([5.2160764e-01, 8.4915513e-01, 7.3328722e-01, 1.1827261e+15],\n",
       "       dtype=float32),\n",
       " array([3.10260355e-01, 2.15019077e-01, 3.10398489e-02, 1.08891796e+15],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_mass_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.74910957, 0.39241824, 0.7223062 ], dtype=float32),\n",
       " array([0.52160764, 0.8491551 , 0.7332872 ], dtype=float32)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rdd = pos_mass_rdd.map(lambda el: el[:3])\n",
    "pos_rdd.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def none_filter(el):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rdd_non_none = pos_rdd.filter(lambda x: x if not np.any(x == None))\n",
    "pos_rdd_non_none.zipWithIndex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_rdd = find_pairs_distr(pos_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
