{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readfof\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import scipy.spatial as SS\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/31 14:13:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"CosmoSparkApplication\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cosmo_data(file_path):\n",
    "\n",
    "    # Read Fof\n",
    "    FoF = readfof.FoF_catalog(\n",
    "        file_path,           # simulation directory\n",
    "        2,                   # snapnum, indicating the redshift (z=1)\n",
    "        long_ids = False,\n",
    "        swap = False,\n",
    "        SFR = False,\n",
    "        read_IDs = False\n",
    "        )\n",
    "\n",
    "    return FoF\n",
    "\n",
    "# Get masses and positions from FoF\n",
    "def get_pos_mass(FoF):\n",
    "\n",
    "    pos = FoF.GroupPos/1e06             # Halo positions in Gpc/h \n",
    "    mass_raw = FoF.GroupMass * 1e10     # Halo masses in Msun/h\n",
    "\n",
    "    dim = pos.shape[0]\n",
    "    id = np.arange(dim, dtype=int).reshape(dim, 1)\n",
    "    pos_mass_matrix = np.hstack([id, pos, mass_raw.reshape(dim, 1)])\n",
    "\n",
    "    return pos_mass_matrix\n",
    "\n",
    "\n",
    "# Mass cut function\n",
    "def mass_filter(pos_mass_rdd, cut):\n",
    "    mass = pos_mass_rdd[4]\n",
    "    if mass >= cut:\n",
    "        return pos_mass_rdd\n",
    "    \n",
    "\n",
    "# Assign each point to a box\n",
    "def assign_box(point, boxes):\n",
    "    id, x, y, z, m = point\n",
    "    box_assign = []\n",
    "    \n",
    "    for box_name, ((x_min, x_max), (y_min, y_max), (z_min, z_max)) in boxes.items():\n",
    "     if (x_min <= x <= x_max) and (y_min <= y <= y_max) and (z_min <= z <= z_max):\n",
    "           box_assign.append((box_name, point))\n",
    "    \n",
    "    return box_assign\n",
    "\n",
    "\n",
    "# KD-tree to find edges in boxes\n",
    "def get_edges(pos_mass_points):\n",
    "    pos_mass_matrix = np.array(pos_mass_points)\n",
    "    pos = pos_mass_matrix[:,1:4]\n",
    "    id = pos_mass_matrix[:,0]\n",
    "\n",
    "    kd_tree = SS.KDTree(pos, leafsize=16, boxsize=1.00001)\n",
    "    edge_idx = kd_tree.query_pairs(r=0.2, output_type=\"ndarray\")\n",
    "    edge_idx = np.array([sorted((id[i], id[j])) for i, j in edge_idx])\n",
    "    \n",
    "    return edge_idx\n",
    "\n",
    "\n",
    "# find unique pairs of edges between 2 boxes (pere uniche)\n",
    "def unique_pears(mat1, mat2):\n",
    "    mat = np.vstack((mat1, mat2))\n",
    "    \n",
    "    return np.unique(mat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pars_file = np.loadtxt(\"/mnt/cosmo_GNN/latin_hypercube_params.txt\", dtype=float)\n",
    "\n",
    "file_path = \"/mnt/cosmo_GNN/Data/\" + str(13)\n",
    "test_FoF = read_cosmo_data(file_path)\n",
    "pos_mass_array = get_pos_mass(test_FoF)\n",
    "\n",
    "# mass cut\n",
    "cut = np.quantile(pos_mass_array[:, 4], 0.997)\n",
    "\n",
    "# parallelize and filter\n",
    "pos_mass_rdd = sc.parallelize(pos_mass_array)\n",
    "\n",
    "pos_mass_filtered = pos_mass_rdd.map(lambda x: mass_filter(x, cut))\\\n",
    "                                .filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_coords, max_coords = calculate_bounds(pos_mass_rdd)\n",
    "min_x, min_y, min_z = 0, 0, 0 #min_coords\n",
    "max_x, max_y, max_z = 1, 1, 1 #max_coords\n",
    "\n",
    "r = 0.1  \n",
    "\n",
    "# Compute the midpoint for every dimension\n",
    "x_mid = np.mean([min_x, max_x])\n",
    "y_mid = np.mean([min_y, max_y])\n",
    "z_mid = np.mean([min_z, max_z])\n",
    "\n",
    "boxes = {\n",
    "    \"box1\": [(min_x    , x_mid + r ), (min_y    , y_mid + r), (min_z    , z_mid + r )],\n",
    "    \"box2\": [(x_mid - r, max_x     ), (min_y    , y_mid + r), (min_z    , z_mid + r )],\n",
    "    \"box3\": [(min_x    , x_mid + r ), (y_mid - r, max_y    ), (min_z    , z_mid + r )],\n",
    "    \"box4\": [(x_mid - r, max_x     ), (y_mid - r, max_y    ), (min_z    , z_mid + r )],\n",
    "    \"box5\": [(min_x    , x_mid + r ), (min_y    , y_mid + r), (z_mid - r, max_z    )],\n",
    "    \"box6\": [(x_mid - r, max_x     ), (min_y    , y_mid + r), (z_mid - r, max_z    )],\n",
    "    \"box7\": [(min_x    , x_mid + r ), (y_mid - r, max_y    ), (z_mid - r, max_z    )],\n",
    "    \"box8\": [(x_mid - r, max_x     ), (y_mid - r, max_y    ), (z_mid - r, max_z    )],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_box_rdd = pos_mass_filtered.flatMap(lambda p: assign_box(p, boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_rdd = point_box_rdd.groupByKey().mapValues(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_rdd = boxes_rdd.mapValues(get_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/31 14:14:24 WARN TaskSetManager: Stage 0 contains a task of very large size (1050 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd_no_key = edges_rdd.map(lambda x: x[1])\\\n",
    "                     .reduce(lambda a, b: unique_pears(a, b))\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  10.],\n",
       "       [  0.,  16.],\n",
       "       [  0.,  33.],\n",
       "       ...,\n",
       "       [727., 738.],\n",
       "       [731., 741.],\n",
       "       [737., 745.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_no_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
