{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readfof\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import scipy.spatial as SS\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"CosmoSparkApplication\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cosmo_data(file_path):\n",
    "\n",
    "    # Read Fof\n",
    "    FoF = readfof.FoF_catalog(\n",
    "        file_path,           # simulation directory\n",
    "        2,                   # snapnum, indicating the redshift (z=1)\n",
    "        long_ids = False,\n",
    "        swap = False,\n",
    "        SFR = False,\n",
    "        read_IDs = False\n",
    "        )\n",
    "\n",
    "    return FoF\n",
    "\n",
    "# Get masses and positions from FoF\n",
    "def get_pos_mass(FoF):\n",
    "\n",
    "    pos = FoF.GroupPos/1e06             # Halo positions in Gpc/h \n",
    "    mass_raw = FoF.GroupMass * 1e10     # Halo masses in Msun/h\n",
    "\n",
    "    dim = pos.shape[0]\n",
    "    id = np.arange(dim, dtype=int).reshape(dim, 1)\n",
    "    pos_mass_matrix = np.hstack([id, pos, mass_raw.reshape(dim, 1)])\n",
    "\n",
    "    return pos_mass_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Mass cut function\n",
    "def mass_filter(pos_mass_rdd, cut):\n",
    "    mass = pos_mass_rdd[4]\n",
    "    if mass >= cut:\n",
    "        return pos_mass_rdd\n",
    "    #cut = pos_mass_rdd.map(lambda x: np.quantile(x[4], quant))\n",
    "    #return pos_mass_rdd.filter(lambda x: x[4] >= cut), cut\n",
    "    \n",
    "#def calculate_bounds(rdd):\n",
    "#    \"\"\"Calcola i limiti dello spazio tridimensionale (min e max per X, Y, Z).\"\"\"\n",
    "#    # Estrai i minimi per X, Y, Z\n",
    "#    min_coords = rdd.map(lambda x: (x[1], x[2], x[3]))\\\n",
    "#                    .reduce(lambda a, b: (min(a[0], b[0]),\n",
    "#                                          min(a[1], b[1]),\n",
    "#                                          min(a[2], b[2])))\n",
    "#    \n",
    "#    # Estrai i massimi per X, Y, Z\n",
    "#    max_coords = rdd.map(lambda x: (x[1], x[2], x[3]))\\\n",
    "#                    .reduce(lambda a, b: (max(a[0], b[0]),\n",
    "#                                          max(a[1], b[1]),\n",
    "#                                          max(a[2], b[2])))\n",
    "#    \n",
    "#    return min_coords, max_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pars_file = np.loadtxt(\"/mnt/cosmo_GNN/latin_hypercube_params.txt\", dtype=float)\n",
    "\n",
    "file_path = \"/mnt/cosmo_GNN/Data/\" + str(13)\n",
    "test_FoF = read_cosmo_data(file_path)\n",
    "pos_mass_array = get_pos_mass(test_FoF)\n",
    "\n",
    "# mass cut\n",
    "cut = np.quantile(pos_mass_array[:, 4], 0.997)\n",
    "\n",
    "# parallelize and filter\n",
    "pos_mass_rdd = sc.parallelize(pos_mass_array)\n",
    "\n",
    "pos_mass_filtered = pos_mass_rdd.map(lambda x: mass_filter(x, cut))\\\n",
    "                                .filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(291009306160011.7)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.00000000e+00, 7.49109566e-01, 3.92418236e-01, 7.22306192e-01,\n",
       "        1.34950329e+15]),\n",
       " array([1.00000000e+00, 5.21607637e-01, 8.49155128e-01, 7.33287215e-01,\n",
       "        1.18272608e+15]),\n",
       " array([2.00000000e+00, 3.10260355e-01, 2.15019077e-01, 3.10398489e-02,\n",
       "        1.08891796e+15]),\n",
       " array([3.00000000e+00, 5.58637619e-01, 8.15573931e-01, 4.94906664e-01,\n",
       "        1.03434979e+15]),\n",
       " array([4.00000000e+00, 4.21333760e-01, 8.44022214e-01, 7.04580367e-01,\n",
       "        1.03251041e+15]),\n",
       " array([5.00000000e+00, 1.06478073e-01, 8.92812669e-01, 2.68124908e-01,\n",
       "        9.60161615e+14]),\n",
       " array([6.00000000e+00, 8.17335069e-01, 5.66343181e-02, 6.80305004e-01,\n",
       "        9.57096015e+14]),\n",
       " array([7.00000000e+00, 3.42625111e-01, 9.14883912e-01, 2.00545162e-01,\n",
       "        9.54643522e+14]),\n",
       " array([8.00000000e+00, 6.79631770e-01, 3.33230466e-01, 3.93127650e-01,\n",
       "        9.50964748e+14]),\n",
       " array([9.00000000e+00, 3.52646112e-01, 8.74759078e-01, 9.50695932e-01,\n",
       "        9.47290135e+14])]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_mass_filtered.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_coords, max_coords = calculate_bounds(pos_mass_rdd)\n",
    "min_x, min_y, min_z = 0, 0, 0 #min_coords\n",
    "max_x, max_y, max_z = 1, 1, 1 #max_coords\n",
    "\n",
    "r = 0.1  \n",
    "\n",
    "# Compute the midpoint for every dimension\n",
    "x_mid = np.mean([min_x, max_x])\n",
    "y_mid = np.mean([min_y, max_y])\n",
    "z_mid = np.mean([min_z, max_z])\n",
    "\n",
    "boxes = {\n",
    "    \"box1\": [(min_x    , x_mid + r ), (min_y    , y_mid + r), (min_z    , z_mid + r )],\n",
    "    \"box2\": [(x_mid - r, max_x     ), (min_y    , y_mid + r), (min_z    , z_mid + r )],\n",
    "    \"box3\": [(min_x    , x_mid + r ), (y_mid - r, max_y    ), (min_z    , z_mid + r )],\n",
    "    \"box4\": [(x_mid - r, max_x     ), (y_mid - r, max_y    ), (min_z    , z_mid + r )],\n",
    "    \"box5\": [(min_x    , x_mid + r ), (min_y    , y_mid + r), (z_mid - r, max_z    )],\n",
    "    \"box6\": [(x_mid - r, max_x     ), (min_y    , y_mid + r), (z_mid - r, max_z    )],\n",
    "    \"box7\": [(min_x    , x_mid + r ), (y_mid - r, max_y    ), (z_mid - r, max_z    )],\n",
    "    \"box8\": [(x_mid - r, max_x     ), (y_mid - r, max_y    ), (z_mid - r, max_z    )],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign each point to a box\n",
    "def assign_box(point, boxes):\n",
    "    id, x, y, z, m = point\n",
    "    box_assign = []\n",
    "    \n",
    "    for box_name, ((x_min, x_max), (y_min, y_max), (z_min, z_max)) in boxes.items():\n",
    "     if (x_min <= x <= x_max) and (y_min <= y <= y_max) and (z_min <= z <= z_max):\n",
    "           box_assign.append((box_name, point))\n",
    "    \n",
    "    return box_assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_box_rdd = pos_mass_filtered.flatMap(lambda p: assign_box(p, boxes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('box6',\n",
       "  array([0.00000000e+00, 7.49109566e-01, 3.92418236e-01, 7.22306192e-01,\n",
       "         1.34950329e+15])),\n",
       " ('box7',\n",
       "  array([1.00000000e+00, 5.21607637e-01, 8.49155128e-01, 7.33287215e-01,\n",
       "         1.18272608e+15])),\n",
       " ('box8',\n",
       "  array([1.00000000e+00, 5.21607637e-01, 8.49155128e-01, 7.33287215e-01,\n",
       "         1.18272608e+15])),\n",
       " ('box1',\n",
       "  array([2.00000000e+00, 3.10260355e-01, 2.15019077e-01, 3.10398489e-02,\n",
       "         1.08891796e+15])),\n",
       " ('box3',\n",
       "  array([3.00000000e+00, 5.58637619e-01, 8.15573931e-01, 4.94906664e-01,\n",
       "         1.03434979e+15])),\n",
       " ('box4',\n",
       "  array([3.00000000e+00, 5.58637619e-01, 8.15573931e-01, 4.94906664e-01,\n",
       "         1.03434979e+15])),\n",
       " ('box7',\n",
       "  array([3.00000000e+00, 5.58637619e-01, 8.15573931e-01, 4.94906664e-01,\n",
       "         1.03434979e+15])),\n",
       " ('box8',\n",
       "  array([3.00000000e+00, 5.58637619e-01, 8.15573931e-01, 4.94906664e-01,\n",
       "         1.03434979e+15])),\n",
       " ('box7',\n",
       "  array([4.00000000e+00, 4.21333760e-01, 8.44022214e-01, 7.04580367e-01,\n",
       "         1.03251041e+15])),\n",
       " ('box8',\n",
       "  array([4.00000000e+00, 4.21333760e-01, 8.44022214e-01, 7.04580367e-01,\n",
       "         1.03251041e+15]))]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "point_box_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punti_partizionati = punti_in_partizioni.groupByKey().mapValues(list)\n",
    "\n",
    "boxes_rdd = point_box_rdd.groupByKey().mapValues(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['box1', 'box5', 'box8', 'box6', 'box4', 'box2', 'box7', 'box3']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/31 12:06:40 WARN TaskSetManager: Stage 104 contains a task of very large size (1050 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('box1',\n",
       "  [array([2.00000000e+00, 3.10260355e-01, 2.15019077e-01, 3.10398489e-02,\n",
       "          1.08891796e+15]),\n",
       "   array([1.50000000e+01, 3.21817935e-01, 3.12683165e-01, 2.90357500e-01,\n",
       "          8.62061609e+14]),\n",
       "   array([3.00000000e+01, 7.56609738e-02, 2.64246106e-01, 1.14312276e-01,\n",
       "          7.42502269e+14]),\n",
       "   array([4.40000000e+01, 4.41150844e-01, 4.86439764e-01, 1.03641391e-01,\n",
       "          6.79963519e+14]),\n",
       "   array([4.50000000e+01, 2.74498075e-01, 2.20003620e-01, 4.95804042e-01,\n",
       "          6.78124132e+14]),\n",
       "   array([4.90000000e+01, 3.99768412e-01, 2.67936915e-01, 8.14262331e-02,\n",
       "          6.68314158e+14]),\n",
       "   array([5.10000000e+01, 5.37295699e-01, 3.17027926e-01, 4.17436182e-01,\n",
       "          6.62795998e+14]),\n",
       "   array([6.20000000e+01, 5.09309411e-01, 2.01037154e-01, 1.82046503e-01,\n",
       "          6.26621569e+14]),\n",
       "   array([6.50000000e+01, 5.61191499e-01, 1.34357795e-01, 3.42717379e-01,\n",
       "          6.21718662e+14]),\n",
       "   array([6.60000000e+01, 5.64165294e-01, 3.13291192e-01, 5.14565229e-01,\n",
       "          6.18651519e+14]),\n",
       "   array([6.90000000e+01, 1.22779042e-01, 3.33618373e-01, 8.43812972e-02,\n",
       "          6.10680462e+14]),\n",
       "   array([7.40000000e+01, 9.43226293e-02, 5.60464084e-01, 3.71035457e-01,\n",
       "          6.00870354e+14]),\n",
       "   array([7.60000000e+01, 3.99585813e-01, 8.07899535e-02, 1.58036157e-01,\n",
       "          5.97191580e+14]),\n",
       "   array([7.80000000e+01, 5.72619796e-01, 1.78903744e-01, 1.42145813e-01,\n",
       "          5.95352193e+14]),\n",
       "   array([9.30000000e+01, 5.41350543e-01, 5.34985922e-02, 3.26747864e-01,\n",
       "          5.60404112e+14]),\n",
       "   array([9.50000000e+01, 3.70666653e-01, 5.74529171e-01, 2.96367139e-01,\n",
       "          5.58564725e+14]),\n",
       "   array([9.60000000e+01, 4.36134964e-01, 2.80751675e-01, 5.28185606e-01,\n",
       "          5.57342672e+14]),\n",
       "   array([9.70000000e+01, 5.97883701e-01, 3.75912413e-02, 4.30078030e-01,\n",
       "          5.57338478e+14]),\n",
       "   array([1.02000000e+02, 4.41740423e-01, 1.54072404e-01, 4.43578660e-01,\n",
       "          5.50594104e+14]),\n",
       "   array([1.06000000e+02, 3.94544393e-01, 6.13227524e-02, 8.64182413e-02,\n",
       "          5.44465052e+14]),\n",
       "   array([1.07000000e+02, 2.19199255e-01, 5.13555855e-02, 2.06875712e-01,\n",
       "          5.43849731e+14]),\n",
       "   array([1.08000000e+02, 4.87299800e-01, 4.00402427e-01, 4.10569280e-01,\n",
       "          5.43849731e+14]),\n",
       "   array([1.12000000e+02, 3.63277853e-01, 5.09630926e-02, 2.91816920e-01,\n",
       "          5.38944710e+14]),\n",
       "   array([1.17000000e+02, 4.01757181e-01, 2.82059796e-02, 4.30431932e-01,\n",
       "          5.34039723e+14]),\n",
       "   array([1.19000000e+02, 5.64795196e-01, 4.81209159e-01, 3.89384389e-01,\n",
       "          5.32200336e+14]),\n",
       "   array([1.37000000e+02, 4.75662738e-01, 4.05385286e-01, 4.08478618e-01,\n",
       "          5.05836586e+14]),\n",
       "   array([1.38000000e+02, 1.98695678e-02, 4.76963446e-02, 3.30507398e-01,\n",
       "          5.05222841e+14]),\n",
       "   array([1.40000000e+02, 5.34344018e-01, 1.97824880e-01, 5.12415707e-01,\n",
       "          5.05222942e+14]),\n",
       "   array([1.44000000e+02, 1.06662236e-01, 3.43500048e-01, 1.16809517e-01,\n",
       "          5.03383488e+14]),\n",
       "   array([1.56000000e+02, 3.94234136e-02, 3.11918139e-01, 2.31333345e-01,\n",
       "          4.81313329e+14]),\n",
       "   array([1.57000000e+02, 2.13728607e-01, 3.47895026e-01, 2.84607947e-01,\n",
       "          4.81310980e+14]),\n",
       "   array([1.62000000e+02, 3.13372374e-01, 4.10187364e-01, 1.66343614e-01,\n",
       "          4.75793625e+14]),\n",
       "   array([1.63000000e+02, 3.81233096e-01, 3.79556894e-01, 2.22538605e-01,\n",
       "          4.75792853e+14]),\n",
       "   array([1.75000000e+02, 5.92465997e-01, 5.44727802e-01, 2.18774617e-01,\n",
       "          4.67209327e+14]),\n",
       "   array([2.01000000e+02, 1.75964087e-01, 3.84950012e-01, 4.74718392e-01,\n",
       "          4.43297231e+14]),\n",
       "   array([2.02000000e+02, 2.55125850e-01, 3.95502336e-02, 4.69957627e-02,\n",
       "          4.42684091e+14]),\n",
       "   array([2.04000000e+02, 2.40248516e-01, 3.71800661e-01, 2.92041898e-01,\n",
       "          4.39618525e+14]),\n",
       "   array([2.09000000e+02, 1.07767597e-01, 4.59671140e-01, 4.40758973e-01,\n",
       "          4.35326611e+14]),\n",
       "   array([2.16000000e+02, 1.07251918e-02, 2.33918935e-01, 2.02240482e-01,\n",
       "          4.31034731e+14]),\n",
       "   array([2.19000000e+02, 5.41277587e-01, 4.74174649e-01, 3.61464359e-02,\n",
       "          4.30421590e+14]),\n",
       "   array([2.21000000e+02, 3.08863044e-01, 1.88787267e-01, 2.16609567e-01,\n",
       "          4.29195344e+14]),\n",
       "   array([2.23000000e+02, 4.67559457e-01, 4.43212539e-01, 8.57621990e-03,\n",
       "          4.28582237e+14]),\n",
       "   array([2.28000000e+02, 1.61899686e-01, 1.27706438e-01, 5.55241942e-01,\n",
       "          4.26742850e+14]),\n",
       "   array([2.36000000e+02, 5.91166437e-01, 8.13453346e-02, 5.12919962e-01,\n",
       "          4.19998477e+14]),\n",
       "   array([2.41000000e+02, 4.19194758e-01, 2.94672489e-01, 8.87477994e-02,\n",
       "          4.18159090e+14]),\n",
       "   array([2.42000000e+02, 3.74725044e-01, 5.72802663e-01, 3.01315457e-01,\n",
       "          4.17546252e+14]),\n",
       "   array([2.48000000e+02, 9.14243683e-02, 8.28711465e-02, 4.86665606e-01,\n",
       "          4.15093490e+14]),\n",
       "   array([2.54000000e+02, 7.28050023e-02, 6.34429008e-02, 3.28014880e-01,\n",
       "          4.10188872e+14]),\n",
       "   array([2.61000000e+02, 8.15196484e-02, 1.86200127e-01, 2.54691571e-01,\n",
       "          4.07736311e+14]),\n",
       "   array([2.66000000e+02, 8.51242393e-02, 1.62280276e-01, 8.75643864e-02,\n",
       "          4.06509729e+14]),\n",
       "   array([2.67000000e+02, 2.11979732e-01, 9.28259939e-02, 3.99635911e-01,\n",
       "          4.06509729e+14]),\n",
       "   array([2.68000000e+02, 1.88593015e-01, 2.54796565e-01, 3.91830146e-01,\n",
       "          4.06509763e+14]),\n",
       "   array([2.73000000e+02, 3.06429677e-02, 2.80379355e-01, 4.15338278e-01,\n",
       "          4.04057034e+14]),\n",
       "   array([2.74000000e+02, 3.43230337e-01, 2.09562346e-01, 2.66801476e-01,\n",
       "          4.03444096e+14]),\n",
       "   array([2.78000000e+02, 3.24484557e-01, 1.80957749e-01, 5.50770462e-01,\n",
       "          4.01604742e+14]),\n",
       "   array([2.80000000e+02, 4.73106876e-02, 5.94619274e-01, 4.00719158e-02,\n",
       "          4.00991602e+14]),\n",
       "   array([2.82000000e+02, 2.80109555e-01, 3.59104604e-01, 1.50834948e-01,\n",
       "          3.99152215e+14]),\n",
       "   array([2.87000000e+02, 5.30250728e-01, 2.49999508e-01, 2.76540935e-01,\n",
       "          3.97925969e+14]),\n",
       "   array([3.02000000e+02, 3.80176395e-01, 8.23827311e-02, 6.45291805e-02,\n",
       "          3.91794735e+14]),\n",
       "   array([3.08000000e+02, 8.28058571e-02, 4.42401081e-01, 1.36820659e-01,\n",
       "          3.89955348e+14]),\n",
       "   array([3.09000000e+02, 1.43162414e-01, 7.80674666e-02, 1.71196848e-01,\n",
       "          3.89955348e+14]),\n",
       "   array([3.12000000e+02, 3.28155309e-01, 8.65847319e-02, 7.96843693e-02,\n",
       "          3.89342241e+14]),\n",
       "   array([3.13000000e+02, 2.93644905e-01, 3.88232470e-02, 4.27158773e-01,\n",
       "          3.88115961e+14]),\n",
       "   array([3.15000000e+02, 2.05279902e-01, 2.33078957e-01, 5.37891626e-01,\n",
       "          3.86276608e+14]),\n",
       "   array([3.17000000e+02, 1.77919760e-01, 1.54320076e-01, 3.41865957e-01,\n",
       "          3.85050361e+14]),\n",
       "   array([3.32000000e+02, 3.83464992e-01, 5.91786027e-01, 5.25478244e-01,\n",
       "          3.79532234e+14]),\n",
       "   array([3.45000000e+02, 4.61236089e-01, 4.36949760e-01, 1.75777625e-03,\n",
       "          3.75853796e+14]),\n",
       "   array([3.50000000e+02, 5.42891473e-02, 8.99397731e-02, 2.82999456e-01,\n",
       "          3.74014107e+14]),\n",
       "   array([3.51000000e+02, 4.12177563e-01, 2.93814242e-01, 1.85245350e-01,\n",
       "          3.74014812e+14]),\n",
       "   array([3.54000000e+02, 3.87411177e-01, 1.60808831e-01, 4.44689125e-01,\n",
       "          3.73400967e+14]),\n",
       "   array([3.55000000e+02, 3.98608178e-01, 1.47565022e-01, 4.35188293e-01,\n",
       "          3.73400967e+14]),\n",
       "   array([3.59000000e+02, 8.02787840e-02, 3.46550822e-01, 6.90038875e-03,\n",
       "          3.72787860e+14]),\n",
       "   array([3.61000000e+02, 3.44041198e-01, 3.10640223e-02, 4.15092289e-01,\n",
       "          3.72173982e+14]),\n",
       "   array([3.64000000e+02, 4.03685719e-02, 4.69069093e-01, 1.30905524e-01,\n",
       "          3.70335367e+14]),\n",
       "   array([3.67000000e+02, 2.86056787e-01, 4.24443990e-01, 5.00063479e-01,\n",
       "          3.69721052e+14]),\n",
       "   array([3.68000000e+02, 5.50382435e-01, 3.14695209e-01, 2.41320357e-01,\n",
       "          3.69722730e+14]),\n",
       "   array([3.70000000e+02, 4.75002192e-02, 5.89633167e-01, 1.76165283e-01,\n",
       "          3.67882840e+14]),\n",
       "   array([3.72000000e+02, 2.86070049e-01, 9.44083333e-02, 4.84930389e-02,\n",
       "          3.67269733e+14]),\n",
       "   array([3.77000000e+02, 3.87537301e-01, 4.82078969e-01, 2.97430784e-01,\n",
       "          3.66043487e+14]),\n",
       "   array([3.78000000e+02, 3.75366718e-01, 4.30966526e-01, 8.25274289e-02,\n",
       "          3.65430347e+14]),\n",
       "   array([3.90000000e+02, 5.97764671e-01, 3.50735754e-01, 5.35475850e-01,\n",
       "          3.61751606e+14]),\n",
       "   array([3.98000000e+02, 1.22440025e-01, 4.78811771e-01, 9.26798284e-02,\n",
       "          3.58073001e+14]),\n",
       "   array([4.07000000e+02, 4.00574245e-02, 4.98160541e-01, 1.68497354e-01,\n",
       "          3.55007233e+14]),\n",
       "   array([4.08000000e+02, 4.81745303e-01, 4.44393158e-01, 2.69450873e-01,\n",
       "          3.55007233e+14]),\n",
       "   array([4.09000000e+02, 5.74138582e-01, 1.76550865e-01, 2.17676252e-01,\n",
       "          3.55006595e+14]),\n",
       "   array([4.11000000e+02, 4.29735959e-01, 4.84014183e-01, 1.76138192e-01,\n",
       "          3.53780986e+14]),\n",
       "   array([4.19000000e+02, 3.75987083e-01, 4.24845904e-01, 7.37287253e-02,\n",
       "          3.50102380e+14]),\n",
       "   array([4.21000000e+02, 4.62225795e-01, 2.75193471e-02, 1.62012964e-01,\n",
       "          3.50102212e+14]),\n",
       "   array([4.23000000e+02, 1.82058513e-01, 1.34972662e-01, 5.62911451e-01,\n",
       "          3.49488938e+14]),\n",
       "   array([4.25000000e+02, 4.10240173e-01, 3.54623944e-01, 4.04922575e-01,\n",
       "          3.49489106e+14]),\n",
       "   array([4.35000000e+02, 5.40259123e-01, 2.95733780e-01, 3.75423700e-01,\n",
       "          3.46423036e+14]),\n",
       "   array([4.37000000e+02, 1.00550599e-01, 1.50400788e-01, 5.83140373e-01,\n",
       "          3.45810366e+14]),\n",
       "   array([4.39000000e+02, 4.58403796e-01, 4.64257002e-01, 2.61126399e-01,\n",
       "          3.45810366e+14]),\n",
       "   array([4.51000000e+02, 4.38437551e-01, 4.37234730e-01, 1.61806986e-01,\n",
       "          3.43356597e+14]),\n",
       "   array([4.53000000e+02, 5.63766837e-01, 4.26149398e-01, 2.44233817e-01,\n",
       "          3.43357738e+14]),\n",
       "   array([4.54000000e+02, 6.14119619e-02, 3.91816378e-01, 5.54716945e-01,\n",
       "          3.42744732e+14]),\n",
       "   array([4.62000000e+02, 5.33263981e-01, 2.15902984e-01, 1.01411201e-01,\n",
       "          3.40292239e+14]),\n",
       "   array([4.72000000e+02, 2.92058336e-03, 4.75686997e-01, 4.18867230e-01,\n",
       "          3.37226605e+14]),\n",
       "   array([4.74000000e+02, 3.06256741e-01, 7.76974857e-02, 8.60250890e-02,\n",
       "          3.36613465e+14]),\n",
       "   array([4.76000000e+02, 4.77837354e-01, 3.66110593e-01, 1.10578395e-01,\n",
       "          3.36613465e+14]),\n",
       "   array([4.77000000e+02, 5.19927859e-01, 1.15933575e-01, 3.62507731e-01,\n",
       "          3.36613465e+14]),\n",
       "   array([4.81000000e+02, 1.24905095e-01, 1.60774842e-01, 5.23550332e-01,\n",
       "          3.34772971e+14]),\n",
       "   array([4.83000000e+02, 3.32325965e-01, 1.42285854e-01, 5.86740017e-01,\n",
       "          3.34774112e+14]),\n",
       "   array([4.88000000e+02, 3.55932385e-01, 6.25628531e-02, 2.91528314e-01,\n",
       "          3.33547496e+14]),\n",
       "   array([4.92000000e+02, 2.98607171e-01, 3.63549888e-02, 4.27672625e-01,\n",
       "          3.32934725e+14]),\n",
       "   array([4.96000000e+02, 5.76453865e-01, 3.84041935e-01, 2.51242459e-01,\n",
       "          3.32934758e+14]),\n",
       "   array([5.04000000e+02, 3.64347965e-01, 2.41996855e-01, 1.64285287e-01,\n",
       "          3.31095338e+14]),\n",
       "   array([5.07000000e+02, 5.55703700e-01, 1.32574365e-01, 3.53606313e-01,\n",
       "          3.30482231e+14]),\n",
       "   array([5.13000000e+02, 8.48298222e-02, 5.37307501e-01, 3.33146527e-02,\n",
       "          3.28642844e+14]),\n",
       "   array([5.22000000e+02, 1.78446606e-01, 4.80979949e-01, 4.67220992e-01,\n",
       "          3.28029738e+14]),\n",
       "   array([5.44000000e+02, 4.90390748e-01, 4.69468504e-01, 3.57923597e-01,\n",
       "          3.24350863e+14]),\n",
       "   array([5.49000000e+02, 5.82653999e-01, 2.54686661e-02, 2.16733858e-01,\n",
       "          3.23124583e+14]),\n",
       "   array([5.53000000e+02, 3.06213051e-01, 1.69041827e-01, 2.69420981e-01,\n",
       "          3.21898269e+14]),\n",
       "   array([5.62000000e+02, 2.00303093e-01, 2.74774760e-01, 2.75915623e-01,\n",
       "          3.20671989e+14]),\n",
       "   array([5.65000000e+02, 5.75394630e-02, 1.56045422e-01, 3.90493959e-01,\n",
       "          3.20058849e+14]),\n",
       "   array([5.67000000e+02, 1.52991757e-01, 1.28121153e-01, 1.27854884e-01,\n",
       "          3.20058849e+14]),\n",
       "   array([5.70000000e+02, 1.71145067e-01, 3.65122259e-01, 3.57659012e-01,\n",
       "          3.19445709e+14]),\n",
       "   array([5.71000000e+02, 5.63944221e-01, 3.12855393e-01, 5.19096673e-01,\n",
       "          3.19444870e+14]),\n",
       "   array([5.73000000e+02, 7.19192578e-03, 2.15382752e-04, 1.23345494e-01,\n",
       "          3.18830186e+14]),\n",
       "   array([5.79000000e+02, 2.58606523e-01, 3.22094262e-01, 5.94997525e-01,\n",
       "          3.18219059e+14]),\n",
       "   array([5.80000000e+02, 3.95808220e-01, 5.86212985e-02, 5.79227984e-01,\n",
       "          3.18219429e+14]),\n",
       "   array([5.82000000e+02, 1.21601939e-01, 5.31112313e-01, 3.86418670e-01,\n",
       "          3.17606255e+14]),\n",
       "   array([5.89000000e+02, 1.31488875e-01, 5.39190710e-01, 1.66244358e-01,\n",
       "          3.15766834e+14]),\n",
       "   array([5.91000000e+02, 1.14431931e-02, 3.26063156e-01, 4.42980379e-01,\n",
       "          3.15153694e+14]),\n",
       "   array([5.92000000e+02, 4.07103598e-01, 2.40706444e-01, 2.84564793e-01,\n",
       "          3.15153694e+14]),\n",
       "   array([5.93000000e+02, 5.79957724e-01, 3.13049436e-01, 2.04776913e-01,\n",
       "          3.14540252e+14]),\n",
       "   array([5.96000000e+02, 3.91050369e-01, 5.66481948e-01, 2.23828852e-01,\n",
       "          3.13927414e+14]),\n",
       "   array([6.07000000e+02, 1.79423273e-01, 2.23079294e-01, 5.20244479e-01,\n",
       "          3.12087960e+14]),\n",
       "   array([6.09000000e+02, 1.07210800e-01, 4.84563380e-01, 4.42119628e-01,\n",
       "          3.11474820e+14]),\n",
       "   array([6.11000000e+02, 5.78883767e-01, 1.97110251e-01, 5.08654475e-01,\n",
       "          3.11474820e+14]),\n",
       "   array([6.14000000e+02, 1.10502221e-01, 1.73270673e-01, 1.39919668e-01,\n",
       "          3.10861680e+14]),\n",
       "   array([6.20000000e+02, 4.52433437e-01, 5.48847616e-01, 1.54828653e-02,\n",
       "          3.10248540e+14]),\n",
       "   array([6.33000000e+02, 2.75077194e-01, 3.80102485e-01, 1.16067104e-01,\n",
       "          3.07182805e+14]),\n",
       "   array([6.38000000e+02, 1.25273630e-01, 2.18417525e-01, 1.03612669e-01,\n",
       "          3.06569330e+14]),\n",
       "   array([6.39000000e+02, 3.49429488e-01, 4.13492620e-01, 3.30341935e-01,\n",
       "          3.05956525e+14]),\n",
       "   array([6.43000000e+02, 3.89989391e-02, 4.92635667e-01, 1.72472313e-01,\n",
       "          3.05343385e+14]),\n",
       "   array([6.45000000e+02, 1.78992420e-01, 4.53965575e-01, 4.89534855e-01,\n",
       "          3.04730245e+14]),\n",
       "   array([6.58000000e+02, 1.27859652e-01, 1.46294534e-01, 5.21475196e-01,\n",
       "          3.02277651e+14]),\n",
       "   array([6.60000000e+02, 3.71956706e-01, 5.81987917e-01, 1.99954987e-01,\n",
       "          3.02277651e+14]),\n",
       "   array([6.62000000e+02, 3.70204866e-01, 2.38029584e-01, 5.90709746e-01,\n",
       "          3.02277651e+14]),\n",
       "   array([6.63000000e+02, 5.77597558e-01, 2.81028092e-01, 2.38407664e-02,\n",
       "          3.02277651e+14]),\n",
       "   array([6.66000000e+02, 2.93599576e-01, 1.46089718e-01, 3.89738977e-01,\n",
       "          3.01664511e+14]),\n",
       "   array([6.77000000e+02, 3.04975271e-01, 7.67175704e-02, 6.60663992e-02,\n",
       "          2.99825090e+14]),\n",
       "   array([6.81000000e+02, 3.89763713e-02, 5.31629384e-01, 4.79660124e-01,\n",
       "          2.98598776e+14]),\n",
       "   array([6.83000000e+02, 1.78995356e-01, 8.85332078e-02, 5.67960322e-01,\n",
       "          2.97985636e+14]),\n",
       "   array([6.85000000e+02, 5.15167475e-01, 3.10065061e-01, 5.95502973e-01,\n",
       "          2.97985636e+14]),\n",
       "   array([6.91000000e+02, 1.77170053e-01, 3.95282745e-01, 3.75837117e-01,\n",
       "          2.97372396e+14]),\n",
       "   array([7.00000000e+02, 2.25917567e-02, 3.06693107e-01, 3.95467460e-01,\n",
       "          2.96146216e+14]),\n",
       "   array([7.01000000e+02, 8.72823317e-03, 5.95679164e-01, 7.87905529e-02,\n",
       "          2.96146216e+14]),\n",
       "   array([7.10000000e+02, 2.80570686e-01, 2.65680939e-01, 2.36971706e-01,\n",
       "          2.94919466e+14]),\n",
       "   array([7.11000000e+02, 5.15548706e-01, 3.35119814e-02, 3.12153548e-01,\n",
       "          2.94918124e+14]),\n",
       "   array([7.19000000e+02, 5.34418672e-02, 4.90027294e-02, 6.10220619e-02,\n",
       "          2.93693387e+14]),\n",
       "   array([7.34000000e+02, 2.46103868e-01, 4.78581458e-01, 5.96092224e-01,\n",
       "          2.92467342e+14]),\n",
       "   array([7.39000000e+02, 3.86513919e-01, 2.41267979e-01, 1.44938469e-01,\n",
       "          2.91854201e+14]),\n",
       "   array([7.43000000e+02, 4.95579123e-01, 4.09274250e-01, 2.78537929e-01,\n",
       "          2.91241061e+14])])]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boxes_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edges(pos_mass_points):\n",
    "    pos_mass_matrix = np.array(pos_mass_points)\n",
    "    pos = pos_mass_matrix[:,1:4]\n",
    "    id = pos_mass_matrix[:,0]\n",
    "    kd_tree = SS.KDTree(pos, leafsize=16, boxsize=1.00001)\n",
    "    edge_idx = kd_tree.query_pairs(r=0.2, output_type=\"ndarray\")\n",
    "    edge_idx = np.array([sorted((id[i], id[j])) for i, j in edge_idx])\n",
    "    return edge_idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_rdd = boxes_rdd.mapValues(get_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('box1',\n",
       "  array([[261., 266.],\n",
       "         [266., 573.],\n",
       "         [ 30., 266.],\n",
       "         ...,\n",
       "         [ 51., 571.],\n",
       "         [571., 685.],\n",
       "         [108., 571.]])),\n",
       " ('box5',\n",
       "  array([[228., 607.],\n",
       "         [228., 315.],\n",
       "         [228., 481.],\n",
       "         ...,\n",
       "         [281., 679.],\n",
       "         [166., 679.],\n",
       "         [166., 281.]])),\n",
       " ('box8',\n",
       "  array([[ 90., 500.],\n",
       "         [ 90., 198.],\n",
       "         [ 36.,  90.],\n",
       "         ...,\n",
       "         [323., 358.],\n",
       "         [323., 746.],\n",
       "         [358., 746.]])),\n",
       " ('box6',\n",
       "  array([[381., 740.],\n",
       "         [ 97., 740.],\n",
       "         [ 89., 740.],\n",
       "         ...,\n",
       "         [465., 541.],\n",
       "         [195., 541.],\n",
       "         [195., 465.]]))]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 2)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_rdd.values().take(2)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_pears(mat1, mat2):\n",
    "    mat = np.hstack((mat1, mat2))\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/08/31 13:39:14 WARN TaskSetManager: Lost task 2.0 in stage 222.0 (TID 922) (10.67.22.240 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n",
      "    vs = list(itertools.islice(iterator, batch))\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1922, in func\n",
      "    yield reduce(f, iterator, initial)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_61555/2651338800.py\", line 2, in <lambda>\n",
      "  File \"/tmp/ipykernel_61555/1046498075.py\", line 2, in unique_pears\n",
      "  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 357, in hstack\n",
      "    arrs = atleast_1d(*tup)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 70, in atleast_1d\n",
      "    result = asanyarray(ary)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "\n",
      "24/08/31 13:39:15 ERROR TaskSetManager: Task 2 in stage 222.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 222.0 failed 4 times, most recent failure: Lost task 2.3 in stage 222.0 (TID 940) (10.67.22.240 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_61555/2651338800.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_61555/1046498075.py\", line 2, in unique_pears\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 357, in hstack\n    arrs = atleast_1d(*tup)\n           ^^^^^^^^^^^^^^^^\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 70, in atleast_1d\n    result = asanyarray(ary)\n             ^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor243.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_61555/2651338800.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_61555/1046498075.py\", line 2, in unique_pears\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 357, in hstack\n    arrs = atleast_1d(*tup)\n           ^^^^^^^^^^^^^^^^\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 70, in atleast_1d\n    result = asanyarray(ary)\n             ^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[284], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m unique_edges_rdd \u001b[38;5;241m=\u001b[39m \u001b[43medges_rdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapValues\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 2\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43munique_pears\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 222.0 failed 4 times, most recent failure: Lost task 2.3 in stage 222.0 (TID 940) (10.67.22.240 executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_61555/2651338800.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_61555/1046498075.py\", line 2, in unique_pears\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 357, in hstack\n    arrs = atleast_1d(*tup)\n           ^^^^^^^^^^^^^^^^\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 70, in atleast_1d\n    result = asanyarray(ary)\n             ^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat jdk.internal.reflect.GeneratedMethodAccessor243.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 274, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/rdd.py\", line 1922, in func\n    yield reduce(f, iterator, initial)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/spark/python/pyspark/util.py\", line 83, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_61555/2651338800.py\", line 2, in <lambda>\n  File \"/tmp/ipykernel_61555/1046498075.py\", line 2, in unique_pears\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 357, in hstack\n    arrs = atleast_1d(*tup)\n           ^^^^^^^^^^^^^^^^\n  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/site-packages/numpy/_core/shape_base.py\", line 70, in atleast_1d\n    result = asanyarray(ary)\n             ^^^^^^^^^^^^^^^\nValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "unique_edges_rdd = edges_rdd.mapValues(lambda x: x)\\\n",
    "                            .reduce(lambda a, b: unique_pears(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['box1',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe508cace60>,\n",
       "       'box5',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50de681d0>,\n",
       "       'box8',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50dd87500>,\n",
       "       'box6',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50e457380>,\n",
       "       'box4',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50dea9dc0>,\n",
       "       'box2',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50de6a450>,\n",
       "       'box7',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50de68dd0>,\n",
       "       'box3',\n",
       "       <pyspark.resultiterable.ResultIterable object at 0x7fe50de6b440>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_edges_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partizioni_parallelizzate = punti_partizionati.partitionBy(punti_partizionati.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def connessione_partizioni(partizione1, partizione2, r):\n",
    "#    \"\"\"\n",
    "#    Trova i collegamenti tra i punti nelle zone di overlap tra due partizioni.\n",
    "#    \"\"\"\n",
    "#    # Estrazione delle coordinate\n",
    "#    coord1 = [(p[1], p[2], p[3]) for p in partizione1]\n",
    "#    coord2 = [(p[1], p[2], p[3]) for p in partizione2]\n",
    "#    \n",
    "#    # Creazione dei KDTree per le due partizioni\n",
    "#    tree1 = KDTree(coord1)\n",
    "#    tree2 = KDTree(coord2)\n",
    "#    \n",
    "#    # Trova i punti di partizione1 che sono vicini a partizione2\n",
    "#    edges = []\n",
    "#    for i, point in enumerate(coord1):\n",
    "#        # Trova tutti i punti in partizione2 entro distanza r da point in partizione1\n",
    "#        indices = tree2.query_ball_point(point, r)\n",
    "#        \n",
    "#        for j in indices:\n",
    "#            # Aggiungi un arco tra il punto di partizione1 e il punto corrispondente di partizione2\n",
    "#            edges.append((partizione1[i], partizione2[j]))\n",
    "#    \n",
    "#    return edges\n",
    "#\n",
    "## Funzione per applicare la connessione in parallelo\n",
    "#def connessione_in_partizione(iterator, r):\n",
    "#    partizioni = list(iterator)\n",
    "#    edges = []\n",
    "#    \n",
    "#    # Connetti i punti tra ogni coppia di partizioni\n",
    "#    for i in range(len(partizioni)):\n",
    "#        for j in range(i + 1, len(partizioni)):\n",
    "#            nome_part1, punti1 = partizioni[i]\n",
    "#            nome_part2, punti2 = partizioni[j]\n",
    "#            edges.extend(connessione_partizioni(punti1, punti2, r))\n",
    "#    \n",
    "#    return iter(edges)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la connessione in parallelo\n",
    "#edges_parallelizzati = partizioni_parallelizzate.mapPartitions(lambda iterator: connessione_in_partizione(iterator, r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.=========>       (7 + 1) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Riduci tutti gli archi ottenuti dalle connessioni parallele\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m grafo_finale \u001b[38;5;241m=\u001b[39m \u001b[43medges_parallelizzati\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/pyspark_env/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Riduci tutti gli archi ottenuti dalle connessioni parallele\n",
    "#grafo_finale = edges_parallelizzati.reduce(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza il grafo finale\n",
    "for edge in grafo_finale:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
