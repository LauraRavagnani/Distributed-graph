{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readfof\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import scipy.spatial as SS\n",
    "from scipy.spatial import KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/08/31 13:54:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"CosmoSparkApplication\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cosmo_data(file_path):\n",
    "\n",
    "    # Read Fof\n",
    "    FoF = readfof.FoF_catalog(\n",
    "        file_path,           # simulation directory\n",
    "        2,                   # snapnum, indicating the redshift (z=1)\n",
    "        long_ids = False,\n",
    "        swap = False,\n",
    "        SFR = False,\n",
    "        read_IDs = False\n",
    "        )\n",
    "\n",
    "    return FoF\n",
    "\n",
    "# Get masses and positions from FoF\n",
    "def get_pos_mass(FoF):\n",
    "\n",
    "    pos = FoF.GroupPos/1e06             # Halo positions in Gpc/h \n",
    "    mass_raw = FoF.GroupMass * 1e10     # Halo masses in Msun/h\n",
    "\n",
    "    dim = pos.shape[0]\n",
    "    id = np.arange(dim, dtype=int).reshape(dim, 1)\n",
    "    pos_mass_matrix = np.hstack([id, pos, mass_raw.reshape(dim, 1)])\n",
    "\n",
    "    return pos_mass_matrix\n",
    "\n",
    "\n",
    "# Mass cut function\n",
    "def mass_filter(pos_mass_rdd, cut):\n",
    "    mass = pos_mass_rdd[4]\n",
    "    if mass >= cut:\n",
    "        return pos_mass_rdd\n",
    "    \n",
    "# Assign each point to a box\n",
    "def assign_box(point, boxes):\n",
    "    id, x, y, z, m = point\n",
    "    box_assign = []\n",
    "    \n",
    "    for box_name, ((x_min, x_max), (y_min, y_max), (z_min, z_max)) in boxes.items():\n",
    "     if (x_min <= x <= x_max) and (y_min <= y <= y_max) and (z_min <= z <= z_max):\n",
    "           box_assign.append((box_name, point))\n",
    "    \n",
    "    return box_assign\n",
    "\n",
    "def get_edges(pos_mass_points):\n",
    "    pos_mass_matrix = np.array(pos_mass_points)\n",
    "    pos = pos_mass_matrix[:,1:4]\n",
    "    id = pos_mass_matrix[:,0]\n",
    "\n",
    "    kd_tree = SS.KDTree(pos, leafsize=16, boxsize=1.00001)\n",
    "    edge_idx = kd_tree.query_pairs(r=0.2, output_type=\"ndarray\")\n",
    "    edge_idx = np.array([sorted((id[i], id[j])) for i, j in edge_idx])\n",
    "    \n",
    "    return edge_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_pars_file = np.loadtxt(\"/mnt/cosmo_GNN/latin_hypercube_params.txt\", dtype=float)\n",
    "\n",
    "file_path = \"/mnt/cosmo_GNN/Data/\" + str(13)\n",
    "test_FoF = read_cosmo_data(file_path)\n",
    "pos_mass_array = get_pos_mass(test_FoF)\n",
    "\n",
    "# mass cut\n",
    "cut = np.quantile(pos_mass_array[:, 4], 0.997)\n",
    "\n",
    "# parallelize and filter\n",
    "pos_mass_rdd = sc.parallelize(pos_mass_array)\n",
    "\n",
    "pos_mass_filtered = pos_mass_rdd.map(lambda x: mass_filter(x, cut))\\\n",
    "                                .filter(lambda x: x is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#min_coords, max_coords = calculate_bounds(pos_mass_rdd)\n",
    "min_x, min_y, min_z = 0, 0, 0 #min_coords\n",
    "max_x, max_y, max_z = 1, 1, 1 #max_coords\n",
    "\n",
    "r = 0.1  \n",
    "\n",
    "# Compute the midpoint for every dimension\n",
    "x_mid = np.mean([min_x, max_x])\n",
    "y_mid = np.mean([min_y, max_y])\n",
    "z_mid = np.mean([min_z, max_z])\n",
    "\n",
    "boxes = {\n",
    "    \"box1\": [(min_x    , x_mid + r ), (min_y    , y_mid + r), (min_z    , z_mid + r )],\n",
    "    \"box2\": [(x_mid - r, max_x     ), (min_y    , y_mid + r), (min_z    , z_mid + r )],\n",
    "    \"box3\": [(min_x    , x_mid + r ), (y_mid - r, max_y    ), (min_z    , z_mid + r )],\n",
    "    \"box4\": [(x_mid - r, max_x     ), (y_mid - r, max_y    ), (min_z    , z_mid + r )],\n",
    "    \"box5\": [(min_x    , x_mid + r ), (min_y    , y_mid + r), (z_mid - r, max_z    )],\n",
    "    \"box6\": [(x_mid - r, max_x     ), (min_y    , y_mid + r), (z_mid - r, max_z    )],\n",
    "    \"box7\": [(min_x    , x_mid + r ), (y_mid - r, max_y    ), (z_mid - r, max_z    )],\n",
    "    \"box8\": [(x_mid - r, max_x     ), (y_mid - r, max_y    ), (z_mid - r, max_z    )],\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "point_box_rdd = pos_mass_filtered.flatMap(lambda p: assign_box(p, boxes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punti_partizionati = punti_in_partizioni.groupByKey().mapValues(list)\n",
    "boxes_rdd = point_box_rdd.groupByKey().mapValues(list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_rdd = boxes_rdd.mapValues(get_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('box1',\n",
       "  array([[261., 266.],\n",
       "         [266., 573.],\n",
       "         [ 30., 266.],\n",
       "         ...,\n",
       "         [ 51., 571.],\n",
       "         [571., 685.],\n",
       "         [108., 571.]])),\n",
       " ('box5',\n",
       "  array([[228., 607.],\n",
       "         [228., 315.],\n",
       "         [228., 481.],\n",
       "         ...,\n",
       "         [281., 679.],\n",
       "         [166., 679.],\n",
       "         [166., 281.]])),\n",
       " ('box8',\n",
       "  array([[ 90., 500.],\n",
       "         [ 90., 198.],\n",
       "         [ 36.,  90.],\n",
       "         ...,\n",
       "         [323., 358.],\n",
       "         [323., 746.],\n",
       "         [358., 746.]])),\n",
       " ('box6',\n",
       "  array([[381., 740.],\n",
       "         [ 97., 740.],\n",
       "         [ 89., 740.],\n",
       "         ...,\n",
       "         [465., 541.],\n",
       "         [195., 541.],\n",
       "         [195., 465.]]))]"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_rdd.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(972, 2)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_rdd.values().take(2)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_pears(mat1, mat2):\n",
    "    mat = np.vstack((mat1, mat2))\n",
    "    return np.unique(mat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_no_key= edges_rdd.map(lambda x: x[1]).reduce(lambda a, b: unique_pears(a, b))    #.flatMap(lambda x: x)\\\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.,  10.],\n",
       "       [  0.,  16.],\n",
       "       [  0.,  33.],\n",
       "       ...,\n",
       "       [727., 738.],\n",
       "       [731., 741.],\n",
       "       [737., 745.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_no_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#partizioni_parallelizzate = punti_partizionati.partitionBy(punti_partizionati.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def connessione_partizioni(partizione1, partizione2, r):\n",
    "#    \"\"\"\n",
    "#    Trova i collegamenti tra i punti nelle zone di overlap tra due partizioni.\n",
    "#    \"\"\"\n",
    "#    # Estrazione delle coordinate\n",
    "#    coord1 = [(p[1], p[2], p[3]) for p in partizione1]\n",
    "#    coord2 = [(p[1], p[2], p[3]) for p in partizione2]\n",
    "#    \n",
    "#    # Creazione dei KDTree per le due partizioni\n",
    "#    tree1 = KDTree(coord1)\n",
    "#    tree2 = KDTree(coord2)\n",
    "#    \n",
    "#    # Trova i punti di partizione1 che sono vicini a partizione2\n",
    "#    edges = []\n",
    "#    for i, point in enumerate(coord1):\n",
    "#        # Trova tutti i punti in partizione2 entro distanza r da point in partizione1\n",
    "#        indices = tree2.query_ball_point(point, r)\n",
    "#        \n",
    "#        for j in indices:\n",
    "#            # Aggiungi un arco tra il punto di partizione1 e il punto corrispondente di partizione2\n",
    "#            edges.append((partizione1[i], partizione2[j]))\n",
    "#    \n",
    "#    return edges\n",
    "#\n",
    "## Funzione per applicare la connessione in parallelo\n",
    "#def connessione_in_partizione(iterator, r):\n",
    "#    partizioni = list(iterator)\n",
    "#    edges = []\n",
    "#    \n",
    "#    # Connetti i punti tra ogni coppia di partizioni\n",
    "#    for i in range(len(partizioni)):\n",
    "#        for j in range(i + 1, len(partizioni)):\n",
    "#            nome_part1, punti1 = partizioni[i]\n",
    "#            nome_part2, punti2 = partizioni[j]\n",
    "#            edges.extend(connessione_partizioni(punti1, punti2, r))\n",
    "#    \n",
    "#    return iter(edges)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applica la connessione in parallelo\n",
    "#edges_parallelizzati = partizioni_parallelizzate.mapPartitions(lambda iterator: connessione_in_partizione(iterator, r))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aaaaaÂ§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.=========>       (7 + 1) / 8]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/anaconda3/envs/pyspark_env/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Riduci tutti gli archi ottenuti dalle connessioni parallele\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m grafo_finale \u001b[38;5;241m=\u001b[39m \u001b[43medges_parallelizzati\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1924\u001b[0m, in \u001b[0;36mRDD.reduce\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m reduce(f, iterator, initial)\n\u001b[0;32m-> 1924\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m vals:\n\u001b[1;32m   1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reduce(f, vals)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/anaconda3/envs/pyspark_env/lib/python3.12/socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:===================================================>       (7 + 1) / 8]\r"
     ]
    }
   ],
   "source": [
    "# Riduci tutti gli archi ottenuti dalle connessioni parallele\n",
    "#grafo_finale = edges_parallelizzati.reduce(lambda a, b: a + b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizza il grafo finale\n",
    "for edge in grafo_finale:\n",
    "    print(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
