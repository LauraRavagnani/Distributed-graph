{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Distributed graph embedding from cosmological simulations**\n",
    "\n",
    "**Group 13**:\n",
    "\n",
    "|student|email|ID|\n",
    "|---|---|---|\n",
    "|Lorenzo Cavezza|lorenzo.cavezza@studenti.unipd.it|2130648|\n",
    "|Giulia Doda|giulia.doda@studenti.unipd.it|2104267|\n",
    "|Giacomo Longaroni|giacomo.longaroni@studenti.unipd.it|2126898|\n",
    "|Laura Ravagnani|laura.ravagnani@studenti.unipd.it|2104271|\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Estimating cosmological parameters from the analysis of cosmic web structures is a key goal in cosmology since it helps us to uncover fundamental properties of the universe. The cosmic web is the large-scale structure of the universe, composed by galaxy clusters and filaments, and some important cosmological parameters from the $\\Lambda \\text{CDM}$ model (the most trusted model nowadays) are for example $\\Omega_{m}$, which is the total matter density, and $\\sigma_{8}$, which measures matter density fluctuations on a scale of $8\\, \\text{Mpc}$ (recalling that $1 \\,\\text{pc} \\sim 3.26 \\,\\text{light years} \\sim 31 \\,\\text{x}\\, 10^{16} \\,\\text{m}$). \n",
    "<div class=\"container\" style=\"display: inline-block;\">\n",
    "  <figure>\n",
    "    <div style=\"padding: 10px; text-align: center;\">\n",
    "      <img src='cosmicweb.png' style=\"width: 450px; height: 250px;\" />\n",
    "      <figcaption><br>Fig. 1: the cosmic web.</b></figcaption>\n",
    "    </div>\n",
    "  </figure>\n",
    "</div>\n",
    "\n",
    "These parameters are traditionally estimated using summary statistics, such as the power spectrum. However, recent advances in machine learning have opened up a promising new approach. For example, **graph neural networks** (GNN) offers many advantages specifically for this task with respect to other types of neural networks (NN). Let’s first recall that a graph consists of a set of nodes connected by edges, that here we consider as symmetrical links between nodes (without a preferred direction). Both nodes and edges can be associated with data. Moreover, we can associate global features with the graph that are related to the entire graph rather than to nodes or edges. A GNN is then a NN that takes graphs as input and can make predictions about node or edge properties, or properties of the graph as a whole. This type of network offers many advantages for the cosmology task.  First of all, it is inherently permutation invariant, and we can design it to respect physical symmetries relevant to our task, such as rotation and translation invariance, so that the network can focus on learning relevant correlations. Another advantage is that, unlike some other network models like convolutional NN (CNN), GNN do not impose any cut in scale.  In CNN, the grid size determines the minimum scale, requiring a cutoff because CNN cannot handle arbitrarily fine grids due to memory limitations. In particular, when dealing with nonlinear scales, CNN need very fine grids, which are not feasible with current computational architectures. Finally the cosmic web can be easily represented as a graph and processing it with a GNN is more appropriate since a graph neural network handle sparse and irregular data, such as the cosmic web, more effectively.\n",
    "\n",
    "Our goal is then to embed the cosmic web from simulated data into a graph so that it can be further processed by a GNN.\n",
    "<div class=\"container\" style=\"display: inline-block;\"> \n",
    "    <figure>\n",
    "      <div style=\"padding: 10px; text-align: center;\">\n",
    "        <img src='graph_8_997.jpeg' style=\"width: 450px; height: 450px;\">\n",
    "        <figcaption><br>Fig. 2: example of a cosmic graph from a simulation represented in 3D space.</b></figcaption>\n",
    "      </div>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "#### The data\n",
    "Quijote is a suite of N-body simulation containing many different catalogues. We focus specifically on the halo catalogue at redshift $z=1$. This provides us with $2000$ dark matter halo simulations, where the halos are already identified by the friends of friends (FoF) algorithm. Each simulations contains $512^3$ particles in a comoving volume of $1\\,\\text{Gpc}$. The simulations are generated through latin hypercube and each of them is associated with different values for the cosmological parameters. \n",
    "\n",
    "#### The method\n",
    "We need to embed each simulation into a graph. First, we find all halo pairs within a distance equal to or less than the linking radius, which we set at $0.2\\,\\text{Gpc} $, following previous studies. Next, we extract the translation and rotation invariant features: the distance between connected halos and two angles that specify the position of each linked halo pair relative to the halos' centroid. It's important to note that all distance calculations apply periodic boundary conditions, to account for the limitations of the simulation box.\n",
    "<div class=\\\"container\\\" style=\\\"display: inline-block;\\\"> \n",
    "    <figure>\n",
    "      <div style=\"padding: 10px; text-align: center;\">\n",
    "        <img src='graph_embedding_space.png' style=\"width: 350px; height: 250px;\">\n",
    "        <figcaption><br>Fig. 3: graph embedding space.</b></figcaption>\n",
    "      </div>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Here we can easily visualize the graph embedding. Each node represents a halo, with its corresponding feature being the halo mass. The graph edges must account for gravitational interactions between halos and respect the required symmetries. Graph edges are defined by: \n",
    "- the linking distance $d$ between linked halos normalized by the linking radius, \n",
    "- the cosines of the two angles describing the position of the linked halo pair relative to the centroid of the halos. \n",
    "\n",
    "Finally, we also assign a global feature to each graph $u = log_{10}N$ where $N$ is the total number of halos in the simulation to summarize the cardinality of the graph.\n",
    "\n",
    "#### Resources and cluster (TO DO BETTER)\n",
    "- 4 virtual machines on CloudVeneto (4 VCPUs, 8GB RAM, 25GB storage)\n",
    "  - VM1: \n",
    "    - hosting the Spark Master and a Spark worker\n",
    "    - with a 250 GB volume mounted on\n",
    "    - network file system (NFS) server \n",
    "  - VM2 $\\rightarrow$ VM4: \n",
    "    - Spark workers\n",
    "    - NFS clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import readfof\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master(\"spark://master:7077\")\\\n",
    "        .appName(\"CosmoSparkApplication\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "def read_cosmo_data(file_path):\n",
    "\n",
    "    # Read Fof\n",
    "    FoF = readfof.FoF_catalog(\n",
    "        file_path,           # simulation directory\n",
    "        2,                   # snapnum, indicating the redshift (z=1)\n",
    "        long_ids = False,\n",
    "        swap = False,\n",
    "        SFR = False,\n",
    "        read_IDs = False\n",
    "        )\n",
    "\n",
    "    return FoF\n",
    "\n",
    "\n",
    "\n",
    "# Get masses and positions from FoF\n",
    "def get_pos_mass(FoF):\n",
    "\n",
    "    pos = FoF.GroupPos/1e06             # Halo positions in Gpc/h \n",
    "    mass_raw = FoF.GroupMass * 1e10     # Halo masses in Msun/h\n",
    "\n",
    "    dim = pos.shape[0]\n",
    "    pos_mass_matrix = np.hstack([pos, mass_raw.reshape(dim, 1)])\n",
    "\n",
    "    return pos_mass_matrix\n",
    "\n",
    "\n",
    "\n",
    "# To assign simulation keys to each point in each simulation\n",
    "def assign_key_to_rows(key_value_pair):\n",
    "\n",
    "    key, array = key_value_pair\n",
    "\n",
    "    return [(key, row) for row in array]\n",
    "\n",
    "\n",
    "\n",
    "# To assign each value of the rdd to each bin\n",
    "def bin_placer(value, edges):\n",
    "\n",
    "    for i in range(1, len(edges)):\n",
    "        if value < edges[i]:\n",
    "            return edges[i - 1]\n",
    "        \n",
    "    return edges[-1]\n",
    "\n",
    "\n",
    "\n",
    "# mass cuts distribution histogram\n",
    "def create_mass_hist(mass_rdd, n_sims):\n",
    "\n",
    "    # bin edges \n",
    "    min_mass_cuts = mass_rdd.map(lambda x: x[1]).reduce(min)\n",
    "    max_mass_cuts = mass_rdd.map(lambda x: x[1]).reduce(max)\n",
    "\n",
    "    # number of bins\n",
    "    num_bins = int(1 + np.log2(n_sims)) # Sturges's rule\n",
    "\n",
    "    # bin width\n",
    "    bin_width = (max_mass_cuts - min_mass_cuts) / num_bins\n",
    "\n",
    "    # edges\n",
    "    edges = [min_mass_cuts + i * bin_width for i in range(int(num_bins) + 1)]\n",
    "\n",
    "    # histogram with map - reduce\n",
    "    mapped_mass_cuts = mass_rdd.map(lambda x: (bin_placer(x[1], edges), 1))\n",
    "    reduced_mass_cuts = mapped_mass_cuts.reduceByKey(lambda x, y: x + y)\n",
    "    mass_cuts_distr = np.array(reduced_mass_cuts.collect())\n",
    "\n",
    "    # create figure\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    ax.hist(mass_cuts_distr[:, 0], bins=11, weights=mass_cuts_distr[:, 1], color='#0f5e6f', alpha=0.5, label='Mass distribution')\n",
    "    ax.set_xlabel('Halo mass ($M_{\\\\odot}$)', fontsize=14)\n",
    "    modot = '$M_{\\\\odot}$'\n",
    "    b_width = \"{:.2e}\".format(bin_width)\n",
    "    ax.set_ylabel(f'Counts  /  {b_width} ' + modot, fontsize=14)\n",
    "    ax.grid(alpha=0.4, linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "# number of halos distribution histogram\n",
    "def create_halos_hist(halos_rdd, n_sims):\n",
    "    \n",
    "    # bin edges \n",
    "    min_n_halos = halos_rdd.map(lambda x: x).reduce(min)\n",
    "    max_n_halos = halos_rdd.map(lambda x: x).reduce(max)\n",
    "\n",
    "    # number of bins\n",
    "    num_bins_halos = int(1 + np.log2(n_sims)) # Sturges's rule\n",
    "\n",
    "    # bin width\n",
    "    bin_width_halos = (max_n_halos - min_n_halos) / num_bins_halos\n",
    "\n",
    "    # edges\n",
    "    edges_halos = [min_n_halos + i * bin_width_halos for i in range(num_bins_halos + 1)]\n",
    "\n",
    "    # histogram with map - reduce\n",
    "    mapped_n_halos = halos_rdd.map(lambda x: (bin_placer(x, edges_halos), 1))\n",
    "    reduced_n_halos = mapped_n_halos.reduceByKey(lambda x, y: x + y)\n",
    "    n_halos_distr = np.array(reduced_n_halos.collect())\n",
    "\n",
    "    # create histogram\n",
    "    fig1, ax1 = plt.subplots(figsize=(10,6))\n",
    "\n",
    "    ax1.hist(n_halos_distr[:, 0], bins=num_bins_halos, weights=n_halos_distr[:, 1], color='#f28a44', alpha=0.5, label='Mass distribution')\n",
    "    ax1.set_xlabel('# of halos', fontsize=14)\n",
    "    b_width = \"{:.2e}\".format(bin_width_halos)\n",
    "    ax1.set_ylabel(f'Counts  /  {b_width} ', fontsize=14)\n",
    "    ax1.grid(alpha=0.4, linestyle='--')\n",
    "\n",
    "\n",
    "\n",
    "# create all the possible halo pairs inside each box\n",
    "def create_pairs(pos_rdd, divisions):\n",
    "\n",
    "    # indexed positions rdd (point indexes)\n",
    "    # --> ( simkey, (point_idx, array(x, y, z)) )\n",
    "    idx_pos_rdd = pos_rdd.groupByKey()\\\n",
    "                         .flatMapValues(lambda vals: enumerate(vals))\n",
    "\n",
    "    # initialize boxes\n",
    "    boxes = sub_box_bounds(divisions, 0.2)\n",
    "\n",
    "    # indexed positions rdd with box assigned\n",
    "    # --> ( simkey_boxkey, (point_idx, array(x, y, z)) )\n",
    "    idx_pos_box_rdd = idx_pos_rdd.flatMapValues(lambda p: assign_box(p, boxes))\\\n",
    "                                 .map(lambda x: (str(x[0]) + '_' + x[1][0], x[1][1]))\n",
    "\n",
    "    # obtain all the possible point pairs for each simulation clustered by boxes\n",
    "    # --> ( (simkey_boxkey, (idx, array)), (simkey_boxkey, (idx, array)) )\n",
    "    cartesian_rdd = idx_pos_box_rdd.groupByKey()\\\n",
    "                            .flatMapValues(lambda points: [(p1,p2) for p1 in points for p2 in points])\\\n",
    "                            .map(lambda x: ((x[0], x[1][0]),(x[0], x[1][1])))\n",
    "\n",
    "    # compute differences between every pair \n",
    "    # --> (simkey_boxkey, (idx1, idx2, coord1, coord2, diff_coord))\n",
    "    diff_rdd = cartesian_rdd.map(lambda x:(x[0][0],(x[0][1][0], x[1][1][0], x[0][1][1],  x[1][1][1] , x[0][1][1] - x[1][1][1])))\n",
    "\n",
    "    # --> (simkey_boxkey, (idx1, idx2, coord1, coord2, diff_coord, norm))\n",
    "    pairs_dist_rdd_with_box = diff_rdd.mapValues(lambda x: (x[0], x[1], x[2], x[3], x[4], np.linalg.norm(x[4])))\n",
    "\n",
    "    pairs_dist_rdd = pairs_dist_rdd_with_box.map(lambda x: (int(x[0].split('_')[0]), (x[1])))\\\n",
    "                                                   .map(convert_to_tuple)\\\n",
    "                                                   .distinct()\\\n",
    "                                                   .map(convert_to_array)\n",
    "    \n",
    "    return pairs_dist_rdd\n",
    "\n",
    "\n",
    "\n",
    "# Plot a graph in 3D space\n",
    "def plot_graph_3D(num, pars_file, pos, masses, edge_idx):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    fontsize = 12\n",
    "\n",
    "    ax = fig.add_subplot(projection=\"3d\")\n",
    "\n",
    "    pos = np.array(pos, dtype=float) * 1.e3   # show in Mpc\n",
    "\n",
    "    # Draw lines for each edge\n",
    "    for (src, dst) in edge_idx:\n",
    "\n",
    "        src = pos[int(src)].tolist()\n",
    "        dst = pos[int(dst)].tolist()\n",
    "\n",
    "        ax.plot([src[0], dst[0]], [src[1], dst[1]], zs=[src[2], dst[2]], linewidth=0.6, color='dimgrey')\n",
    "\n",
    "    # Plot nodes\n",
    "    mass_mean = np.mean(masses)\n",
    "    for i,m in enumerate(masses):\n",
    "            ax.scatter(pos[i, 0], pos[i, 1], pos[i, 2], s=50*m*m/(mass_mean**2), zorder=1000, alpha=0.6, color='mediumpurple')\n",
    "\n",
    "    ax.xaxis.set_tick_params(labelsize=fontsize)\n",
    "    ax.yaxis.set_tick_params(labelsize=fontsize)\n",
    "    ax.zaxis.set_tick_params(labelsize=fontsize)\n",
    "\n",
    "    ax.set_xlabel('x (Mpc)', fontsize=16, labelpad=15)\n",
    "    ax.set_ylabel('y (Mpc)', fontsize=16, labelpad=15)\n",
    "    ax.set_zlabel('z (Mpc)', fontsize=16, labelpad=15)\n",
    "\n",
    "    rl = '$R_{link} = 0.2$'\n",
    "\n",
    "    pars_file = pars_file[num]\n",
    "\n",
    "    ax.set_title(f'\\tGraph n°{num}, Masses $\\\\geq 99.7$% percentile, {rl} Mpc \\t \\n \\n $\\\\Omega_m = {float(pars_file[0]):.3f}$ \\t $\\\\sigma_8 = {float(pars_file[1]):.3f}$', fontsize=20)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def get_edge_attributes(pos_rdd, linked_pairs_dist_rdd):\n",
    "    # centroids positions\n",
    "    halo_centroids = pos_rdd.reduceByKey(lambda x, y: (x + y) / 2)\n",
    "\n",
    "    # joined rdd with halo centroids positions\n",
    "    joined_rdd = linked_pairs_dist_rdd.join(halo_centroids)\n",
    "\n",
    "    # distance between each point from each pair and halo centroid\n",
    "    row_col_diff_rdd = joined_rdd.mapValues(\n",
    "        lambda x: (\n",
    "            x[0][0],        # idx_i\n",
    "            x[0][1],        # idx_j\n",
    "            x[0][2] - x[1], # row\n",
    "            x[0][3] - x[1], # col\n",
    "            x[0][4],        # diff\n",
    "            x[0][5]         # dist\n",
    "            ))\n",
    "\n",
    "    # normalizing \n",
    "    normalized_rdd = row_col_diff_rdd.mapValues(\n",
    "        lambda x: (\n",
    "            x[0],                      # idx_i\n",
    "            x[1],                      # idx_j\n",
    "            x[2]/np.linalg.norm(x[2]), # row_normalized\n",
    "            x[3]/np.linalg.norm(x[3]), # col_normalized\n",
    "            x[4]/np.linalg.norm(x[4]), # s_ij\n",
    "            x[5]/0.2                   # |d_ij|/r \n",
    "        )\n",
    "    )\n",
    "\n",
    "    # edge attributes\n",
    "    edge_attr_rdd = normalized_rdd.mapValues(\n",
    "        lambda x: (\n",
    "            x[0],\n",
    "            x[1],\n",
    "            np.dot( x[2].T, x[3] ), # cos(alpha)\n",
    "            np.dot( x[2].T, x[4] ), # cos(beta)\n",
    "            x[5]                    # |d_ij|/r \n",
    "        )\n",
    "    )\n",
    "\n",
    "    return edge_attr_rdd\n",
    "\n",
    "\n",
    "\n",
    "# Graph object\n",
    "class graph:\n",
    "\n",
    "    def __init__(self, node_f, pos, sim_pars, glob_f, edge_idx, edge_f):\n",
    "        \n",
    "        self.node_f = node_f\n",
    "        self.pos = pos\n",
    "        self.sim_pars = sim_pars\n",
    "        self.glob_f = glob_f\n",
    "        self.edge_idx = edge_idx\n",
    "        self.edge_f = edge_f\n",
    "\n",
    "\n",
    "\n",
    "# Create graph object\n",
    "def create_graph(element):\n",
    "\n",
    "    sim_graph = graph(\n",
    "        np.array(element[0])[:,3],   # node_f = masses\n",
    "        np.array(element[0])[:,0:3], # pos\n",
    "        np.array(element[3]),        # sim_pars\n",
    "        np.array(element[2]),        # glob_f\n",
    "        np.array(element[1])[:,0:2], # edge_idx\n",
    "        np.array(element[1])[:,2:5], # edge_f\n",
    "    )\n",
    "\n",
    "    return(sim_graph)\n",
    "\n",
    "\n",
    "\n",
    "# Function that returns the partitions bounds as a dictionary of lists of tuples, \n",
    "# each tuple being the min and max of a dimension\n",
    "def sub_box_bounds(box_number, r_link):\n",
    "\n",
    "    sub_length = 1.0 / box_number # partition length\n",
    "    bounds = {}\n",
    "    base = 'box'\n",
    "    sub_box_counter = 1\n",
    "\n",
    "    for x in range(0, box_number):\n",
    "        for y in range(0, box_number):\n",
    "            for z in range(0, box_number):\n",
    "                key = base + str(sub_box_counter)\n",
    "                single_bounds = []\n",
    "                centre = [x, y, z] # vertex of a sub_box corresponding to min x,y,z\n",
    "                for i in range(3):\n",
    "                    min_bound = round(max(0, centre[i] * sub_length - 0.5 * r_link), 2)\n",
    "                    max_bound = round(min(1, centre[i] * sub_length + sub_length + 0.5 * r_link), 2)\n",
    "                    single_bounds.append((min_bound, max_bound))\n",
    "                bounds[key] = single_bounds\n",
    "                sub_box_counter += 1\n",
    "\n",
    "    return bounds\n",
    "\n",
    "\n",
    "\n",
    "# Assign each point to a box\n",
    "def assign_box(point, boxes):\n",
    "\n",
    "    position = point[1]\n",
    "    x, y, z = position\n",
    "    box_assign = []\n",
    "    \n",
    "    for box_name, ((x_min, x_max), (y_min, y_max), (z_min, z_max)) in boxes.items():\n",
    "     if (x_min <= x <= x_max) and (y_min <= y <= y_max) and (z_min <= z <= z_max):\n",
    "           box_assign.append((box_name, point))\n",
    "    \n",
    "    return box_assign\n",
    "\n",
    "\n",
    "\n",
    "# Convert all element of an rdd into a tuple\n",
    "def convert_to_tuple(data):\n",
    "\n",
    "    return (\n",
    "        data[0],\n",
    "        data[1][0],\n",
    "        data[1][1],\n",
    "        (float(data[1][2][0]), float(data[1][2][1]), float(data[1][2][2])),  # from array to tuple\n",
    "        (float(data[1][3][0]), float(data[1][3][1]), float(data[1][3][2])),  # from array to tuple\n",
    "        (float(data[1][4][0]), float(data[1][4][1]), float(data[1][4][2])),  # from array to tuple\n",
    "        float(data[1][5])             \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Convert vectors of an rdd into a np.array (key, ( ...)) \n",
    "def convert_to_array(data):\n",
    "    \n",
    "    return (\n",
    "        data[0],\n",
    "        (\n",
    "            data[1],\n",
    "            data[2],\n",
    "            np.array([float(data[3][0]), float(data[3][1]), float(data[3][2])]),  # from tuple to array\n",
    "            np.array([float(data[4][0]), float(data[4][1]), float(data[4][2])]),  # from tuple to array\n",
    "            np.array([float(data[5][0]), float(data[5][1]), float(data[5][2])]),  # from tuple to array\n",
    "            float(data[6]) # to standard float\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulations parameter\n",
    "sim_pars_file = np.loadtxt(\"/mnt/cosmo_GNN/latin_hypercube_params.txt\", dtype=float)\n",
    "\n",
    "# number of simulations to be processed\n",
    "n_sims = 2000\n",
    "\n",
    "# path list with simulation keys\n",
    "path_list = [(i, \"/mnt/cosmo_GNN/Data/\" + str(i)) for i in range(n_sims)]\n",
    "\n",
    "# parallelize path list and read files\n",
    "fof_rdd = sc.parallelize(path_list)\\\n",
    "            .mapValues(read_cosmo_data)\n",
    "\n",
    "# get positions and masses for each point\n",
    "pos_mass_rdd = fof_rdd.mapValues(get_pos_mass)\\\n",
    "                      .flatMap(assign_key_to_rows)\n",
    "\n",
    "# cut percentile\n",
    "cut = 0.997\n",
    "\n",
    "# get mass cuts \n",
    "mass_cut_rdd = fof_rdd.mapValues(get_pos_mass)\\\n",
    "                      .mapValues(lambda x: np.quantile(x[:, -1], cut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_mass_hist(mass_cut_rdd, n_sims=n_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect mass cuts into numpy array\n",
    "mass_cuts = mass_cut_rdd.values().collect()\n",
    "mass_cuts = np.array(mass_cuts)\n",
    "\n",
    "# filter by mass\n",
    "pos_mass_rdd_filtered = pos_mass_rdd.filter(lambda x: x[1][-1] >= mass_cuts[x[0]])\n",
    "\n",
    "# number of halos in each simulation\n",
    "n_halos = pos_mass_rdd_filtered.countByKey()\n",
    "\n",
    "# number of halos rdd\n",
    "n_halos_rdd = sc.parallelize(n_halos.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_halos_hist(n_halos_rdd, n_sims=n_sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# positions rdd ---> (simkey, pos)\n",
    "pos_rdd = pos_mass_rdd_filtered.mapValues(lambda x: x[:3])\n",
    "\n",
    "# create pairs\n",
    "pairs_dist_rdd = create_pairs(pos_mass_rdd_filtered, divisions=4)\n",
    "\n",
    "# filter by linking radius\n",
    "linked_pairs_dist_rdd = pairs_dist_rdd.filter(lambda x: x[1][-1] <= 0.2)\n",
    "\n",
    "# pairs rdd --> (simkey, (idx1, idx2)) --> reverse pair already included\n",
    "pairs_rdd = linked_pairs_dist_rdd.mapValues(lambda x: (x[0], x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve data for a simulation to show the plot\n",
    "sim_num_plot = 8\n",
    "pos_mass_rdd_filtered_plot = pos_mass_rdd_filtered.filter(lambda x: x[0] == sim_num_plot)\\\n",
    "                                                  .map(lambda x: x[1])\n",
    "pos_mass_plot = np.array(pos_mass_rdd_filtered_plot.collect())\n",
    "pos_plot = pos_mass_plot[:, :3]\n",
    "mass_plot = pos_mass_plot[:, 3]\n",
    "pairs_idx_plot = pairs_rdd.filter(lambda x: x[0] == sim_num_plot)\\\n",
    "                          .values()\n",
    "pairs_idx_plot_array = np.array(pairs_idx_plot.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edge attributes\n",
    "edge_attr_rdd = get_edge_attributes(pos_rdd, linked_pairs_dist_rdd)\n",
    "\n",
    "# group by simulation\n",
    "grouped_idx_pos_rdd = pos_mass_rdd_filtered.groupByKey()\\\n",
    "                                 .mapValues(list)\n",
    "\n",
    "grouped_edge_rdd = edge_attr_rdd.groupByKey()\\\n",
    "                                .mapValues(list)\n",
    "\n",
    "# parallelize simulation parameters file and global features\n",
    "param_rdd = sc.parallelize([(i, el) for i, el in enumerate(sim_pars_file)])\n",
    "u = sc.parallelize([(i[0], math.log10(i[1])) for i in n_halos.items()])\n",
    "\n",
    "# graph rdd (a graph for each simulation)\n",
    "# masses, positions, simulation parameters, global features, edge indexes, edge features\n",
    "raw_graph_rdd = grouped_idx_pos_rdd.join(grouped_edge_rdd)\\\n",
    "                                   .join(u)\\\n",
    "                                   .join(param_rdd)\\\n",
    "                                   .mapValues(lambda x: (x[0][0][0], x[0][0][1], x[0][1], x[1]))\n",
    "\n",
    "graph_rdd = raw_graph_rdd.mapValues(lambda x: create_graph(x))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_rdd.collect()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
